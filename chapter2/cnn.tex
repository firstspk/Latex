Convolution neural network (CNN)\textsuperscript{\cite{cnn}} คือโมเดลปัญญาประดิษฐ์ประเภทหนึ่งมักจะนำมาใช้กับงานที่เกี่ยวกับการจำแนกวัตถุในภาพ เช่น แมว หมา มนุษย์ รถ เป็นต้น
การที่ CNN สามารถจำแนกภาพออกมาได้ว่าเป็นหมวดหมู่อะไรนั้นต้องผ่านชั้นตัวกรอง (filter) หรือเคอร์เนล (kernel), pooling layer, fully connected layer 
และใช้ softmax หรือ logistic function เพื่อสกัดคุณลักษณะ (extract feature) สำหรับใช้ในการจำแนกว่าเป็นวัตถุหมวดหมู่อะไร ดังรูปที่ \ref{fig:CNN architecture}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\textwidth]{chapter2/images/CNN.png}
		\caption[ตัวอย่างโครงสร้างของ CNN ที่ใช้ในการจำแนกหมวดหมู่ของวัตถุ]{ตัวอย่างโครงสร้างของ CNN ที่ใช้ในการจำแนกหมวดหมู่ของวัตถุ\textsuperscript{\cite{cnn}}}
    	\label{fig:CNN architecture}
\end{figure}

\subsubsection{ตัวกรอง/เคอร์เนล}
ตัวกรองหรือเคอร์เนล คือชั้นที่ใช้ในการสกัดคุณลักษณะของรูปภาพออกมาด้วยสี่เหลี่ยมเล็กๆขนาด NxN โดยที่ N$\in$[1, 2, 3, ...] ดังรูปที่ \ref{fig:kernel_3x3} 
และสมมติให้ภาพที่ใช้ในการประมวลผมเป็นดังรูปที่ \ref{fig:input_ex}
\begin{figure}[!ht]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=0.4\textwidth]{chapter2/images/kernel_ex.png}
		\caption{ตัวอย่างเคอร์เนลขนาด 3x3}
		\label{fig:kernel_3x3}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
		\includegraphics[width=0.5\textwidth]{chapter2/images/input_ex.png}
		\caption{ตัวอย่างภาพที่ใช้ในการประมวลผล}
        \label{fig:input_ex}
	\end{subfigure}
	\caption{ตัวอย่างเคอร์เนล และภาพที่ใช้ในการประมวลผล}
	\label{fig:kernel_input_ex}
\end{figure}
\clearpage
เมื่อนำเคอร์เนล (รูปที่ \ref{fig:kernel_3x3}) ไปทาบกับภาพ (รูปที่ \ref{fig:input_ex}) แล้วคูณค่าในเคอร์เนลกับพิกเซล (pixel) ที่ทาบจะได้คุณลักษณะของช่องนั้นจากนั้นเลื่อนต่อไปจนครบทั้งรูป 
ซึ่งระยะในการเลื่อนนั้นขึ้นอยู่กับผู้สร้างว่าต้องการจะให้เลื่อนครั้งละกี่ช่อง แต่ระยะการเลื่อนที่มากขึ้นจะทำให้ความสัมพันธ์ของคุณลักษณะที่ได้ออกมาน้อยลง โดยการวางเคอร์เนลเทียบบนภาพนั้นจะวางไม่ให้เกินกรอบรูป 
แต่ถ้าต้องการทาบกับทุกพิกเซลในภาพสามารถทำได้ด้วยการให้พื้นที่ที่เกินขอบภาพไปเท่ากับ 0 เทคนิคนี้เรียกว่า padding และคุณลักษณะที่ได้ออกมาทั้งหมดจะเรียกว่าผังคุณลักษณะ (features map) ตามรูปที่ \ref{fig:example feature map}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.5\textwidth]{chapter2/images/feature_map.png}
		\caption[ตัวอย่างการหาผังคุณลักษณะ]{ตัวอย่างการหาผังคุณลักษณะ\textsuperscript{\cite{cnn}}}
    	\label{fig:example feature map}
\end{figure}

\subsubsection{Pooling}
Pooling คือชั้นที่สามารถลดขนาดของภาพลงเพื่อลดข้อมูลที่ไม่จำเป็นลง ซึ่งมีหลายประเภทแต่นิยมใช้มีสองประเภทได้แก่ max pooling และ average pooling
โดยที่ max pooling จะใช้ในการหาค่าที่มากที่สุดในเคอร์เนลที่ทาบอยู่ดังรูปที่ \ref{fig:example max pooling} ในขณะที่ average pooling 
จะหาค่าเฉลี่ยของภายในเคอร์เนลออกมาดังรูปที่ \ref{fig:example average pooling}
\begin{figure}[!ht]
	\centering
	\begin{subfigure}[b]{1.0\textwidth}
		\centering
		\includegraphics[width=0.5\textwidth]{chapter2/images/max_pooling.png}
		\caption{ตัวอย่างการทำ max pooling\textsuperscript{\cite{cnn}}}
		\label{fig:example max pooling}
    \end{subfigure}
    \begin{subfigure}[b]{1.0\textwidth}
        \centering
		\includegraphics[width=0.5\textwidth]{chapter2/images/average_pooling.png}
		\caption{ตัวอย่างการทำ average pooling\textsuperscript{\cite{average_pooling}}}
		\label{fig:example average pooling}
	\end{subfigure}
	\caption{ตัวอย่างการใช้ max pooling และ average pooling กับภาพ}
	\label{fig:pooling_ex}
\end{figure}
\clearpage

\subsubsection{Activation function\textsuperscript{\cite{activation}}}
ในแต่ละชั้นของ CNN นั้นจะมี activation function เป็นสิ่งที่กำหนดว่าผลลัพธ์ของชั้นนั้นว่าจะอยู่ในช่วงไหน 
เช่น -1 ถึง 1, 0 ถึง 1 ขึ้นอยู่กับฟังก์ชันที่ใช้ ซึ่งจะแบ่งออกเป็นสองแบบคือ แบบเป็นเส้นตรงและแบบไม่เป็นเส้นตรง

แบบเป็นเส้นตรงจะมีสมการของฟังก์ชันดังนี้ $ f(x) = x $ โดยที่ $x$ คือข้อมูลป้อนเข้าของฟังก์ชัน ทำให้ผลลัพธ์ที่ได้จากฟังก์ชันนี้มีค่าอยู่ในช่วง -inf ถึง inf
ส่วนในแบบไม่เป็นเส้นตรงจะมีหลายฟังก์ชัน แต่ฟังก์ชันที่เป็นที่นิยมใช้คือ Rectified Linear Unit หรือ ReLU, Leaky ReLU และ Parametric ReLU โดยทั้งสามฟังก์ชันมีสมการดังนี้\\
\textbf{ReLU}\\
$ f(x) =
\begin{cases}
	0 & \text{if } x < 0,\\
	x & \text{otherwise}
\end{cases}
$\\
\textbf{Leaky ReLU}\\
$ f(x) =
\begin{cases}
	0.01x & \text{if } x < 0,\\
	x & \text{otherwise}
\end{cases}
$\\
\textbf{Parametric ReLU} โดยที่ $\alpha\in\mathbb{R^+}$\\
$ f(x) =
\begin{cases}
	\alpha x & \text{if } x < 0,\\
	x & \text{otherwise}
\end{cases}
$
\subsubsection{Fully connected layer}
เป็นชั้นที่จะรวมคุณลักษณะทั้งหมดของชั้นก่อนหน้าให้เป็นเวกเตอร์ (vector) ก่อนที่จะนำไปคำนวณด้วย activation function เพื่อหาคำตอบสำหรับการแยกหมวดหมู่ของภาพ
ซึ่งฟังก์ชันที่นิยมใช้จะมีสองฟังก์ชัน คือ softmax และ sigmoid (logistic) โดยทั้งสองมีความแตกต่างกันดังตารางที่ \ref{tab:softmax_vs_sigmoid}
\begin{table}[!ht]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		{} & Softmax & Sigmoid\\
		\hline\hline
		1 & \begin{tabular}{@{}c@{}}สมการคือ $f(x_i) = \frac{exp(x_i)}{\sum_{j=0}^k exp(x_j)}$ \\โดยที่ k คือจำนวนข้อมูล และ i$\in${0, 1, 2, ..., k}\end{tabular} & $f(x_i) = \frac{1}{1+exp(-x_i)}$ โดยที่ i$\in${0, 1, 2, ..., k}\\
		\hline
		2 & ผลรวมของความน่าจะเป็นจะเท่ากับ 1 เสมอ & ผลรวมของความน่าจะเป็นไม่จำเป็นต้องเท่ากับ 1\\
		\hline
		3 & มักใช้ในการจำแนกหมวดหมู่มากกว่าสองหมวดหมู่ขึ้นไป & มักใช้ในการจำแนกหมวดหมู่เพียงสองหมวดหมู่\\
		\hline
	\end{tabular}
	\caption{ตารางแสดงการเปรียบเทียบระหว่าง softmax function และ sigmoid function}
	\label{tab:softmax_vs_sigmoid}
\end{table}
\clearpage

\subsubsection{Intersection over union (IoU)}
\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.3]{chapter2/images/iou_equation.png}
		\caption[ภาพแสดงการหา IoU ด้วยการใช้คำตอบจริงของการทำนายและกรอบสี่เหลี่ยมที่ได้จากการทำนาย]{ภาพแสดงการหา IoU ด้วยการใช้คำตอบจริงของการทำนายและกรอบสี่เหลี่ยมที่ได้จากการทำนาย\textsuperscript{\cite{iou_pic}}}
    	\label{fig:iou_equation}
\end{figure}
เป็นวิธีในการทดสอบประสิทธิภาพของการตรวจจับวัตถุ โดยค่า IoU นั้นสามารถหาได้จากการนำกรอบสี่เหลี่ยมจริงของวัตถุและกรอบสี่เหลี่ยมที่ได้จากการทำนาย
มาหาอัตราส่วนระหว่างพื้นที่ที่กรอบสี่เหลี่ยมทั้งสองทับซ้อนกัน และหารด้วยพื้นที่ทั้งหมดของกรอบสี่เหลี่ยมทั้งสองรวมกัน ซึ่งสามารถเขียนในรูปสมการได้ดังนี้
\begin{equation}
IoU(P,G) = \frac{\left| P \cap G \right|}{\left| P \cup  G \right|}					
\end{equation}
โดยที่
\begin{conditions}
IoU			&  ค่าที่ใช้สำหรับวัดผลความใกล้เคียงระหว่างสองกรอบสี่เหลี่ยม    \\
P			&  พื้นที่ของกรอบสี่เหลี่ยมที่ทำนายได้	\\
G			&  พื้นที่ของกรอบสี่เหลี่ยมจริงของรูปภาพ					\\
\end{conditions}

\subsubsection{Area under the curve (AUC) - Receiver Operating Characteristics Curve (ROC)}
\input{chapter2/auc}

\subsubsection{Non Maximum Suppression (NMS)}
Non Maximum Suppression\textsuperscript{\cite{nms}} คือ อัลกอริทึม (algorithm) ที่นิยมใช้เข้ามาช่วยจัดการปัญหากรอบสี่เหลี่ยมที่ซ้อนทับกันซึ่งเกิดจากการทำนายซ้ำ เพื่อให้ได้กรอบสี่เหลี่ยมที่บ่งบอกถึงตำแหน่งของวัตถุนั้นเพียงกรอบเดียว 
\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.3]{chapter2/images/NMS.png}
		\caption[ตัวอย่างการทำงานของ NMS]{ตัวอย่างการทำงานของ NMS\textsuperscript{\cite{nms_pic}}}
    	\label{fig:NMS}
\end{figure}

\subsubsection{Optimization algorithm}
ในขั้นตอนการสร้างโมเดลปัญญาประดิษฐ์นั้นสิ่งที่สำคัญคือ การลดความผิดพลาด (error) หรือค่าคอสท์ (cost) นั่นทำให้ต้องมีเครื่องมือสำหรับปรับพารามิเตอร์ของโมเดลเพื่อให้มีความผิดพลาดน้อยที่สุด
นั่นทำให้มีการคิดค้น optimization algorithm ต่างๆขึ้นมา ตัวอย่างเช่น gradient descent (GD), Momentum, RMS Prop และ Adam เป็นต้น
ซึ่งเมทริกซ์ weight มักจะสุ่มค่าในตอนต้น แล้วใช้ optimization algorithm ในการลดค่าคอสท์ที่ได้จาก cost funtion พร้อมทั้งหาเมทริกซ์ weight และค่าอคติ (bias) ที่ดีที่สุด
โดยจากตัวอย่างที่ยกไปก่อนหน้านี้แต่ละอัลกอริทึมนั้นมีรายละเอียดดังนี้
\begin{enumerate}
	\item Gradient descent (GD) เป็นอัลกอริทึมที่จะทำการปรับเมทริกซ์ weight และค่าอคติของทุกข้อมูลที่ใช้ในการสร้างโมเดล โดยจะมีสมการดังนี้
	\begin{equation}
		W = W - \alpha \frac{\partial J(W)}{\partial W}
	\end{equation}
	\begin{equation}
		b = b - \alpha \frac{\partial J(b)}{\partial b}
	\end{equation}
	โดยที่
	\begin{conditions}
		W & เมทริกซ์ weight\\
		\alpha & อัตราการเรียนรู้ (learning rate)\\
		J & cost function\\
		b & ค่าอคติ
	\end{conditions}
	สมการแรกนั้นคือสมการการปรับเมทริกซ์ weight ในขณะที่สมการที่สองคือสมการปรับค่าอคติ ซึ่งตั้งสองสมการนั้นจะมีอัตราการเรียนรู้เป็นตัวกำหนดอัตราการเปลี่ยนแปลง จากรูปที่ \ref{fig:gd_explain}
	จุดสีแดงหมายถึงกรณีที่ weight มีค่าน้อยกว่าค่าที่เหมาะสมในจุดที่ม่วง จากรูปจะเห็นว่าความชัน ณ จุดสีแดงมีค่าเป็นลบทำให้เมื่อนำไปใส่ในสมการแล้วจะทำให้มีการปรับ weight ให้เพิ่มขึ้น 
	ในทางตรงกันข้ามที่จุดสีน้ำเงินจะมีความชันเป็นบวกทำให้มีการปรับ weight ให้น้อยลง ซึ่งการปรับแต่ละครั้งจะมีระยะก้าวกว้างขนาดไหนจะขึ้นอยู่กับอัตร่การเรียนรู้ที่ได้กำหนดไว้ก่อนหน้านี้ 
	\begin{figure}[!ht]
		\centering
		\includegraphics[width=0.3\textwidth]{chapter2/images/gd_explain.png}
			\caption{กราฟสำหรับอธิบายการทำงานของ gradient descent}
			\label{fig:gd_explain}
	\end{figure} 
	ข้อเสียของ gradient descent คือจะทำการปรับค่าพารามิเตอร์หลังจากทำการคำนวณจนครบทุกข้อมูลในชุดข้อมูลที่ใช้ในการสร้างแล้วเท่านั้น ทำให้เกิดปัญญาหน่วยความจำไม่เพียงพอเมื่อชุดข้อมูลมีขนาดที่ใหญ่มาก
	จึงทำให้เกิดเทคนิคที่เรียกว่า stochastic gradient descent (SGD) กล่าวคือถ้าหาก gradient descent ทำการคำนวณจนครบทุกข้อมูลจึงจะมีการปรับพารามิเตอร์ stochastic gradient descent 
	จะคำนวณเพียงข้อมูลหนึ่งตัวหรือเป็นชุดของข้อมูล (subset) ซึ่งหากใช้เป็นชุดของข้อมูลจะเรียกว่า minibatch stochastic gradient descent การทำแบบนี้จะทำให้ลดการใช้หน่วยความจำลง 
	และทำให้การสร้างโมเดลนั้นทำได้เร็วขึ้นกว่าการใช้ gradient descent
	\item Momentum หรือ gradient descent with momentum คืออัลกอริทึมที่จะเร่งความเร็วในการปรับลดค่าจาก cost function โดยใช้สิ่งที่เรียกว่า moving average โดยสามารถหาได้จากสมการดังนี้
	\begin{equation}
		V_{dW} = \beta V_{dW} + (1 - \beta)dW
	\end{equation}
	\begin{equation}
		V_{db} = \beta V_{db} + (1 - \beta)db
	\end{equation}
	โดยที่
\end{enumerate}
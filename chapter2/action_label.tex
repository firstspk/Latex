การจำแนกการกระทำเป็นกระบวนการในการทำนายการกระทำของมนุษย์หรือสิ่งที่สนใจอื่นๆที่เกิดการกระทำขึ้นภายในวิดีโอ 
โดยในหัวข้อนี้จะกล่าวถึงตั้งแต่ขั้นตอนการได้มาซึ่งชุดข้อมูลมีกระบวนการอย่างไร การนำโมเดลปัญญาประดิษฐ์มาใช้ในการจำแนกการกระทำ และการวัดผลของโมเดลปัญญาประดิษฐ์
โดยชุดข้อมูลที่ผู้วิจัยได้เลือกนำมาศึกษาจากชุดข้อมูลที่ถูกเป็นที่กล่าวถึงในปัจจุบัน และมีขนาดของชุดข้อมูลที่ใหญ่

จากบทความข้างต้นชุดข้อมูลที่เราได้เลือกนำมาใช้ได้แก่ YouTube-8M ,AVA ,Moment in Time โดยแต่ละชุดข้อมูลจะมีความแตกต่างกันในหลายๆด้าน 
แต่จะมีสิ่งที่เหมือนกัน คือ เป็นชุดข้อมูลสำหรับการวิเคราะห์ผลวิดีโอที่มีการสนใจการกระทำของมนุษย์ โดยในบทความนี้จะกล่าวถึงความแตกต่างในด้านต่างๆ 
เช่น เป้าหมายของแต่ละชุดข้อมูล ,วิธีการเก็บข้อมูลสำหรับชุดข้อมูล ,วิธีการสร้างคำกำกับ และรายละเอียดของชุดข้อมูล จากนั้นจะสรุปข้อมูลของแต่ละชุดข้อมูล

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{YouTube-8M} 
\begin{enumerate}
	\item {ชุดข้อมูล}
	\begin{enumerate}
		\setlength\itemsep{-0.25em}
		\item เป้าหมายของชุดข้อมูล : ใช้ทำนายธีมของวิดีโอ
		\item จำนวนของวิดีโอ : 8,264,650 วิดีโอ
		\item ความยาวเฉลี่ยของแต่ละวิดีโอ : 229.6 วินาที
		\item จำนวนของหมวดหมู่ของคำกำกับ : 4800 หมวดหมู่
		\item กฏในการรวบรวมข้อมูลดังนี้
		\begin{enumerate}
			\setlength\itemsep{-0.25em}
			\item ทุกๆหัวข้อต้องเป็นรูปธรรม
			\item ในแต่ละหัวข้อต้องมีจำนวนวิดีโอไม่น้อยกว่า 200 วิดีโอ
			\item ความยาวของวิดีโอต้องอยู่ระหว่าง 120 - 500 วินาที
		\end{enumerate}
		หลังจากได้กฏในการรวบรวมข้อมูลแล้ว ขั้นตอนต่อไปคือการสร้างคำศัพท์ที่ใช้ในการค้นหาข้อมูลวิดีโอจากใน YouTube 
		\item ขั้นตอนในการสร้างคำศัพท์มีดังนี้
		\begin{enumerate}
			\setlength\itemsep{-0.25em}
			\item กำหนดรายการที่อนุญาตหัวข้อที่เป็นรูปธรรมมา 25 ชนิด เช่น เกมส์ เป็นต้น
			\item กำหนดบัญชีดำหัวข้อที่คิดว่าไม่เป็นรูปธรรมไว้ เช่น software เป็นต้น
			\item รวบรวมหัวข้อที่มีอยู่ในรายการที่อนุญาตอย่างน้อย 1 หัวข้อ และต้องไม่มีอยู่ในบัญชีดำซึ่งจะทำให้ได้หัวข้อที่ต้องการมาประมาณ 50,000 หัวข้อ
			\item จากนั้นใช้ผู้ประเมินจำนวน 3 คน ในการคัดหัวข้อที่คิดว่าเป็นรูปธรรม และสามารถจดจำหรือเข้าใจได้ง่ายโดยไม่ต้องเชี่ยวชาญในด้านนั้นๆ 
			ซึ่งผู้ประเมิน ก็จะมีคำถามว่า “มันยากขนาดไหนถึงจะระบุได้ว่ามีหัวข้อดังกล่าวอยู่ในรูปหรือวิดีโอ โดยใช้เพียงแค่การมองเท่านั้น?” โดยแบ่งเป็นระดับดังนี้
			\begin{enumerate}
				\setlength\itemsep{-0.25em}
				\item บุคคลทั่วไปสามารถเข้าใจได้
				\item บุคคลทั่วไปที่ผ่านการอ่านบทความที่เกี่ยวข้องมาแล้วสามารถเข้าใจได้
				\item ต้องเชี่ยญในด้านใดซักด้านจึงจะเข้าใจได้
				\item เป็นไปไม่ได้ ถ้าไม่มีความรู้ที่ไม่ได้เป็นรูปธรรม
				\item ไม่เป็นรูปธรรม
			\end{enumerate}
			\item หลังจากคำถามข้างบนและการให้คะแนน จะทำการเก็บไว้เฉพาะหัวข้อที่มีคะแนนเฉลี่ยมากที่สุดอยู่ที่ประมาณ 2.5 คะแนนหรือต่ำกว่าเท่านั้น
			\item ทำให้สุดท้ายเหลือเพียงประมาณ 10,000 หัวข้อที่สามารถใช้ได้
			\item หลังจากได้หัวข้อที่คิดว่าเป็นรูปธรรมแล้วก็นำไปค้นหาและรวบรวมด้วย YouTube annotation system โดยมีขั้นตอนดังนี้										
			\begin{enumerate}
				\setlength\itemsep{-0.25em}
				\item สุ่มเลือกวิดีโอมา 10 ล้านวิดีโอ พร้อมกับหัวข้อของวิดีโอ โดยใช้กฏที่กำหนดไว้ เอาหัวข้อที่มีจำนวนวิดีโอน้อยกว่า 200 วิดีโอออก
				\item ทำให้เหลือจำนวนวิดีโออยู่ 8,264,650 วิดีโอ
				\item แยกออกเป็น 3 ส่วน Train set, Validate set และ Test set ในอัตราส่วน 70:20:10 ตามลำดับ
			\end{enumerate}
		\end{enumerate}
	\end{enumerate}
	\item {โมเดลปัญญาประดิษฐ์}
	\begin{enumerate}
		\setlength\itemsep{-0.25em}
		\item การเตรียมข้อมูล
			\begin{enumerate}  
				\item คุณลักษณะระดับเฟรม : การลดขนาดของข้อมูล เนื่องจากมีข้อมูลที่มีขนาดใหญ่ทำให้ใช้เวลาในการเปิดนาน ซึ่งกระบวนการนี้จะมีการลดความเร็วเฟรมต่อวินาที 
				เวกเตอร์ของคุณลักษณะ และแปลงข้อมูลจาก 32 บิท ให้เป็น 8 บิท
				\item คุณลักษณะระดับวิดีโอ : การแยกเวกเตอร์คุณลักษณะระดับวิดีโอจากคุณลักษณะระดับเฟรมซึ่งการทำแบบนี้ทำให้ได้ประโยชน์ 3 ข้อ 
				คือโมเดลทั่วไปที่ไม่ใช่โครงข่ายประสาทเทียใสามารถนำไปใช้งานได้ ขนาดข้อมูลเล็กลง และเหมาะกับการนำไปสร้างโมเดล domain adaptive มากขึ้น
			\end{enumerate}	
		\item โมเดลปัญญาประดิษฐ์ที่ใช้ในการทดสอบชุดข้อมูลแบบที่เป็นคุณลักษณะระดับเฟรม
			\begin{enumerate}
				\setlength\itemsep{-0.25em}
				\item one vs all logistic regression classifier + average pooling
				\item Deep bag of frames (แนวคิดแบบเดียวกับ deep bag of words)
				\item Long short-term memory (LSTM)
			\end{enumerate}
		\item โมเดลปัญญาประดิษฐ์ที่ใช้ในการทดสอบชุดข้อมูลแบบที่เป็นคุณลักษณะระดับวิดีโอ
			\begin{enumerate}
				\setlength\itemsep{-0.25em}
				\item Logistic regression
				\item Support vector machine (SVM)
				\item Mixture of Expert (MoE)
			\end{enumerate}
		\item เครื่องมือที่ใช้วัดผลสำหรับงานวิจัยนี้ คือ
			\begin{enumerate}
				\setlength\itemsep{-0.25em}
				\item Mean Average Precision (mAP)
				\item Hit@k (เหมือนกันกับ Top@k)
				\item Precision at equal recall rate (PERR)
			\end{enumerate}
		\item ความสามารถของโมเดลปัญญาประดิษฐ์ที่ใช้ชุดข้อมูลของ YouTube-8M ในการสร้างเทียบกับชุดข้อมูลสำหรับทดสอบของ YouTube-8M
			\begin{table}[!ht]
				\centering
				\begin{tabular}{|c|c|c|c|c|}
					\hline
					{Input features} & {Modeling approach} & {mAP} & Hit@1 & PERR\\
					\hline
					\multirow{3}{*}{Frame-level} & Logistic + average & 11.0 & 50.8 & 42.2\\
					& Deep bag of frames & 26.9 & 62.7 & 55.1\\
					& LSTM & 26.6 & \textbf{64.5} & \textbf{57.3}\\
					\hline
					\multirow{3}{*}{Video-level} & SVM & 17.0 & 56.3 & 47.9\\
					& Logistic regression & 28.1 & 60.5 & 53.0\\
					& Mixture-of-2-experts & \textbf{30.0} & 63.3 & 55.8\\
					\hline
				\end{tabular}
				\caption{ผลการทดสอบโมเดลต่างๆบนชุดข้อมูลที่ใช้ในการทดสอบของ YouTube-8M}
				\label{tab: youtube_youtube}
			\end{table}
			\item ความสามารถของโมเดลปัญญาประดิษฐ์ที่ใช้ชุดข้อมูลของ YouTube-8M ในการสร้างเทียบกับชุดข้อมูลสำหรับทดสอบของ Sports-1M
			\begin{table}[!ht]
				\centering
				\begin{tabular}{|c|c|c|c|}
					\hline
					{Approach} & {mAP} & Hit@1 & PERR\\
					\hline
					Logistic regression & 58.0 & 60.1 & 79.6\\
					Mixture-of-2-experts & 61.3 & 63.2 & 82.6\\
					LSTM & 67.6 & 65.7 & 86.2\\
					\hline
					Hierarchical 3D convolutions\textsuperscript{\cite{karpathy2014large}} & - & 61.0 & 80.0\\
					Stacked 3D convolutions\textsuperscript{\cite{yue2015beyond}} & - & 61.0 & 85.0\\
					LSTM with optical flow and pixels\textsuperscript{\cite{tran2014c3d}} & - & \textbf{73.0} & \textbf{91.0}\\
					\hline
				\end{tabular}
				\caption{ผลการทดสอบโมเดลต่างๆบนชุดข้อมูลที่ใช้ในการทดสอบของ YouTube-8M}
				\label{tab: youtube_youtube}
			\end{table}
		\item ความสามารถของโมเดลปัญญาประดิษฐ์ที่ใช้ชุดข้อมูลของ YouTube-8M ในการสร้างเทียบกับชุดข้อมูลสำหรับทดสอบของ ActivityNet
			\begin{table}[!ht]
				\centering
				\begin{tabular}{|c|c|c|c|}
					\hline
					{Approach} & {mAP} & Hit@1 & Hit@5\\
					\hline
					Mixture-of-2-experts & \textbf{77.6} & \textbf{74.9} & \textbf{91.6}\\
					LSTM & 57.9 & 63.4 & 81.0\\
					\hline
					Ma, Bargal et al.\textsuperscript{\cite{ma2017less}} & 53.8 & - & -\\
					Heilbron et al.\textsuperscript{\cite{caba2015activitynet}} & 43.0 & - & -\\
					\hline
				\end{tabular}
				\caption{ผลการทดสอบโมเดลต่างๆบนชุดข้อมูลที่ใช้ในการทดสอบของ YouTube-8M}
				\label{tab: youtube_youtube}
			\end{table}
		\item ปัญหาที่พบ\\
		เนื่องจากว่า YouTube-8M นั้นมีจำนวนข้อมูลที่เยอะมาก ทำให้ไม่สามารถตรวจสอบความถูกต้องของชุดข้อมูลได้ทั้งหมดว่ามีความถูกต้องมากน้อยขนาดไหน 
		ทำให้อาจเกิดข้อผิดพลาดได้ (ปัจจุบันปี 2019 YouTube-8M ได้มีการตรวจสอบข้อมูลอีกครั้ง เพื่อเพิ่มประสิทธิภาพของชุดข้อมูลซึ่งทำให้ปัจจุบันจำนวนข้อมูล 
		และจำนวนหัวข้อลดน้อยลงจากข้อมูลที่ใช้อ้างอิงในบทความข้างต้นที่ได้กล่าวมา)
	\end{enumerate}	
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\subsubsection*{Atomic visual action (AVA)}	
\begin{enumerate}
	\item {ชุดข้อมูล}
	\begin{enumerate}
		\item เป้าหมายของชุดข้อมูล : สนใจการกระทำของมนุษย์เป็นศูนย์กลาง
		\item จำนวนของวิดีโอ : 640 วิดีโอ
		\item ความยาวเฉลี่ยของแต่ละวิดีโอ : 15 นาที และถูกสุ่มตัวอย่างด้วยความถี่ 1 hz 
		\item จำนวนของหมวดหมู่ : 80 หมวดหมู่
		\item ขั้นตอนการเก็บข้อมูลสำหรับการทำชุดข้อมูลมีขั้นตอนการทำ 5 ขั้นดังนี้
		\begin{enumerate}
			\item การสร้างคำศัพท์การกระทำจะมีหลักการ 3 ข้อในการรวบรวมคำศัพท์ดังนี้
			\begin{enumerate}
				\item เก็บรวบรวมคำศัพท์ทั่วไปที่เกิดขึ้นในชีวิตประจำวัน
				\item จะต้องมีเอกลักษณ์สามารถเห็นได้ชัดเจน เช่น การถือของ
				\item กำหนดรูปแบบของคำศัพท์ขึ้นมา และใช้ความรู้จากชุดข้อมูลอื่นในการทำให้ได้หมวดหมู่การกระทำของมนุษย์ที่ครอบคลุม
			\end{enumerate}
			\item ภาพยนต์และส่วนที่เลือกมาใช้ทำชุดข้อมูล AVA ทั้งหมดจะถูกนำมาจาก YouTube โดยเริ่มจากการรวบรวมเอารายชื่อของนักแสดงที่มีชื่อเสียง
			ซึ่งจะมีความหลากหลายของเชื้อชาติรวมกันอยู่ วิดีโอที่ถูกคัดเลือกจะมีเกณฑ์ดังนี้
			\begin{enumerate}
				\item วิดีโอต้องอยู่ในหมวด ภาพยนต์ และละครโทรทัศน์
				\item วิดีโอจะต้องมีความยาวมากกว่า 30 นาที
				\item อัพโหลดมาแล้วเป็นระยะเวลาอย่างน้อย 1 ปี
				\item มียอดวิวคนดูมากกว่า 1000 ครั้ง
				\item ละเว้นวิดีโอบางประเภท เป็นภาพขาว-ดำ ,มีความละเอียดต่ำ ,การ์ตูน หรือวิดีโอเกม
			\end{enumerate}
			\item การสร้างกรอบสี่เหลี่ยมครอบมนุษย์ที่อยู่ภายในภาพประกอบด้วย 2 ขั้นตอน
			\begin{enumerate}
				\item สร้างกรอบสี่เหลี่ยมโดยใช้โมเดลปัญญาประดิษฐ์ faster RCNN สำหรับการตรวจจับมนุษย์
				\item ใช้มนุษย์ในการตรวจสอบและแก้ไขกรอบสี่เหลี่ยมที่ผิดพลาด
			\end{enumerate}	
			\item การเชื่อมบุคคลในช่วงระยะเวลาสั้นๆของเฟรม\\
			ทำการเชื่อมกรอบสี่เหลี่ยมที่อยู่ในช่วงเวลาเดียวกันด้วยวิธีการทำนายตำแหน่งโดยยึดมนุษย์เป็นศูนย์กลาง ซึ่งจะนำมาคำนวณความใกล้เคียงกันของคู่กรอบสี่เหลี่ยม 
			และใช้ person embedding จากนั้นจะใช้อัลกอริทึม Hungarian ในการหาตัวเลือกที่ดีที่สุด
			\item การสร้างคำกำกับ\\
			การสร้างคำกำกับของการกระทำจะใช้มนุษย์ในการสร้าง ซึ่งจะใช้หน้าต่างโปรแกรมสำหรับช่วยเหลือในการสร้างซึ่งในหนึ่งกรอบสี่เหลี่ยม สามารถมีคำกำกับของการกระทำได้สูงสุดถึง 7 คำ 
			นอกจากนั้นสามารถตั้งสถานะเนื้อหาที่ไม่เหมาะสม หรือกรอบสี่เหลี่ยมที่ผิดพลาดได้อีกด้วย ในทางปฎิบัติจะสังเกตได้ว่ามันมีโอกาสผิดอย่างหลีกเลี่ยงไม่ได้ 
			เมื่อต้องได้รับคำสั่งให้หาคำกำกับของการกระทำที่ถูกต้องจาก 80 หมวดหมู่ จึงแบ่งขั้นตอนออกเป็น 2 ขั้นตอนดังนี้
			\begin{enumerate}
				\setlength\itemsep{-0.25em}
				\item สร้างข้อเสนอสำหรับคำกำกับของการกระทำจากนั้นจับกลุ่มเข้าด้วยกัน ซึ่งจะทำให้มีโอกาสถูกต้องมากกว่าเป็นคำกำกับแยกเดี่ยว
				\item ผู้ตรวจสอบข้อเสนอจะตรวจสอบข้อเสนอที่ได้จากขั้นตอนแรก ซึ่งในแต่ละวิดีโอจะใช้มนุษย์ในการตรวจสอบ 3 คน 
				เมื่อคำกำกับของการกระทำถูกตรวจสอบด้วยผู้ตรวจสอบข้อเสนออย่างน้อย 2 คน คำกำกับของการกระทำนั้นจะถูกยึดเป็นคำกำกับหลัก
			\end{enumerate}
		\end{enumerate}
	\end{enumerate}
	\item {โมเดลปัญญาประดิษฐ์}
	\begin{enumerate}
		\item โมเดลปัญญาประดิษฐ์ที่งานวิจัยนี้ใช้ คือ two stream variant ซึ่งจะทำการประมวลผลทั้ง RGB flow และ optical flow 
		โดยเป็นโครงสร้างของ faster RCNN ที่นำ inception network เข้ามาใช้
		\item เครื่องมือที่ใช้วัดผลสำหรับงานวิจัยนี้ คือค่า IoU และ 3D IoUs 
		\begin{enumerate}
			\item ค่า IoU คือค่าที่ใช้วัดความสอดคล้องระหว่างสองเฟรมใช้สำหรับการวัดผลระดับเฟรม โดยจะเป็นหาพื้นที่ร่วมกันระหว่างกรอบสี่เหลี่ยมที่ตรวจเจอและกรอบสี่เหลี่ยมจริงของวัตถุ
			\item ค่า 3D IoUs คือค่าที่ใช้วัดความสอดคล้องระหว่างสองวิดีโอใช้สำหรับการวัดผลระดับวิดีโอโดยเทียบกันระหว่างตำแหน่งคำตอบจริงในช่วงของเฟรมที่ต่อกัน (ground-truth tubes) 
			และตำแหน่งคำตอบจากการตรวจจับในช่วงของเฟรมที่ต่อกัน (linked detection tubes) ซึ่งตำแหน่งคำตอบจริงในช่วงของเฟรมที่ต่อกัน หมายถึงการนำเอากรอบสี่เหลี่ยมจริงของวัตถุในเฟรมที่ติดกันมาเรียงต่อกัน 
			และตำแหน่งคำตอบจากการตรวจจับในช่วงของเฟรมที่ต่อกัน หมายถึงการนำเอากรอบสี่เหลี่ยมที่ตรวจเจอมาเรียงต่อกันในเฟรมที่ติดกันมาเรียงต่อกัน
		\end{enumerate}
		\item ความสามารถของโมเดลปัญญาประดิษฐ์ในปัจจุบัน
		\begin{enumerate}
			\item จากการทดสอบการเทียบโมเดลปัญญาประดิษฐ์ของงานวิจัยนี้และวิธีการอื่นๆ โดยนำไปทดสอบกับชุดข้อมูลวิดีโอ JHMDB และ UCF101-24 ได้ผลลัพธ์ออกมาดังนี้
			\begin{table}[!ht]
				\centering
				\begin{tabular}{|c|c|c|c|}
					\hline
					{Frame-mAP}&{JHMDB}&{UCF101-24}\\
					\hline
					Actionness 			& 39.9				& 	-						\\
					Peng w/o MR			& 56.9				& 64.8						\\
					Peng w/  MR 		& 58.5				& 65.7						\\
					ACT					& 65.7				& 69.5						\\
					\hline
					Out approach		& \textbf{73.3}		& \textbf{76.3}				\\
					\hline
				\end{tabular}
				\caption{ผลการทดลองของวิธีต่างๆบนคุณลักษณะระดับเฟรม}
				\label{tab: transfer learning}
			\end{table}
		\end{enumerate}
		\item ปัญหาที่พบ
		ในปัจจุบันยังไม่มีโมเดลปัญญาประดิษฐ์ที่ทดสอบด้วยชุดข้อมูล AVA และได้ผลการทำงานที่ดี เนื่องจากชุดข้อมูลนี้สนใจการกระทำของมนุษย์ที่มีรายละเอียดเล็กๆน้อยๆ 
		ทำให้ยากต่อการทำนายสำหรับโมเดลปัญญาประดิษฐ์
	\end{enumerate}
\end{enumerate}
\clearpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{Moment in Time}
\begin{enumerate}
	\item {ชุดข้อมูล}
	\begin{enumerate}
		\setlength\itemsep{-0.25em}
		\item เป้าหมายของชุดข้อมูล : สนใจการกระทำทุกการกระทำในวิดีโอ เช่น การกระทำของ คน สัตว์ สิ่งของ และ ปรากฎการณ์ธรรมชาติ 
		\item จำนวนของวิดีโอ : >1,000,000 วิดีโอ
		\item ความยาวเฉลี่ยของแต่ละวิดีโอ : 3 วินาที
		\item จำนวนของหมวดหมู่ : 339 หมวดหมู่
		\item วิธีการเก็บรวบรวมข้อมูล : 
	\begin{enumerate}
		\item เริ่มจากการรวบรวมคำ (verb) ที่มีการใช้อยู่ทั่วไปในชีวิตประจำวันมา 4,500 คำจาก VerbNet จากนั้นนำมาแบ่งกลุ่มคำ(verb) ที่มีความหมายใกล้เคียงกันโดยใช้ features จาก Propbank และ FrameNet โดยเก็บข้อมูลเป็นแบบ binary feature vector ซึ่งถ้าคำ (verb) ไหนมีความเกี่ยวข้องกับ feature ก็จะให้ค่าเป็น 1 ถ้าไม่เกี่ยวข้องกันจะให้ค่าเป็น 0 จากนั้นจึงใช้วิธี k-means clustering ในการแบ่งกลุ่ม เมื่อแบ่งกลุ่มแล้วจากนั้นจะเลือกคำ (verb) จากในแต่ละกลุ่มนั้น โดยคำ (verb) ที่เลือกมานั้นจะเป็นที่ใช้บ่อยที่สุดในกลุ่มนั้น และลบคำ (verb) นั้นออกจากกลุ่มทั้งหมด (คำ ๆ หนึ่งสามารถอยู่ได้หลายกลุ่ม) จากนั้นจะทำกระบวนการนี่ไปเรื่อย ๆ แต่คำ (verb) ที่เลือกมาจะต้องไม่มีความหมายคลุมเครือ ไม่สามารถมองเห็นหรือได้ยินได้ และต้องไม่มีความหมายเหมือนกับคำ (verb) ที่เคยเลือกมาก่อน จนสุดท้ายแล้วได้ออกมาที่ 339 class
		\item ต่อมาทำการหาชุดข้อมูลวิดีโอโดยจะตัดออกมาเพียง 3 วินาทีที่เกี่ยวข้องกับคำ (verb) ใน 339 class ที่เลือกมา จากวิดีโอ แหล่งต่างกัน 10 แหล่ง การตัดวิดีโอนั้นจะไม่ใช้พวก Video2Gif (โมเดลที่ระบุตำแหน่งของสิ่งที่น่าสนใจในวิดีโอ) เพราะจะทำให้เกิด bias ขึ้นจะเกิดขึ้นตอนสร้างโมเดลจากนั้นจะทำการส่งข้อมูลของคำ (verb) และวิดีโอที่ตัดไปยัง Amazon Mechanical Turk (AMT หรือตลาดแรงงาน) เพื่อทำการ label โดยพนักงานแต่ละคนของ AMT จะได้ 64 วิดีโอซึ่งเกี่ยวข้องกับคำ (verb) หนึ่ง และอีก 10 วิดีโอที่มีการทำ label อยู่แล้ว โดยวิดีโอที่มีการทำ label ถ้ามีพนักงานของ AMT ตอบเหมือนกันกับที่ทำ label ไว้เกิน 90\% ถึงจะนำเข้าไปรวมกับชุดข้อมูลส่วนอีก 64 วิดีโอถ้าเป็นของ training set จะต้องผ่านพนักงานของ AMT อย่างน้อย 3 ครั้ง และต้อง label เหมือนกัน 75\% ขึ้นไปถึงจะถือว่าเป็น label ที่ถูกต้อง ถ้าเป็นของ validation และ test set จะต้องผ่านพนักงานของ AMT อย่างน้อย 4 ครั้ง และต้อง label เหมือนกัน 85\% ขึ้นไป ที่ไม่ตั่งเกณฑ์ไว้ที่ 100\% เพราะจะทำให้วิดีโอนั้นยากเกินไปที่จะทำให้สามารถจำการกระทำได้	
	\end{enumerate}
\end{enumerate}
	\item การเตรียมข้อมูล
		\begin{enumerate}
			\item training set จะมี 802,264 วิดีโอ และมีวิดีโอในแต่ละ class อยู่ที่ 500 ถึง 5,000 วิดีโอ
			\item validation set จะมี 33,900 วิดีโอ และมีวิดีโอในแต่ละ class อยู่ที่ 100 วิดีโอ
			\item เริ่มการ preprocess จากแยกภาพRGB ออกมาจากวิดีโอ และทำการเปลี่ยนขนาดของภาพให้เป็น 340x256  pixel
			\item ใช้ TVL1 optical flow algorithm จาก opencv เพื่อลดข้อมูลรบกวนที่จะเกิดขึ้น
			\item ทำการแปลงค่าที่อยู่ใน optical flow ให้เป็นเลขจำนวนเต็ม(integer) เพื่อทำให้การคำนวณนั้นเร็วยิ่งขึ้น
			\item ปรับค่า displacement ใน optical flow ให้ค่าสูงสุดเป็น 15 ต่ำสุดเป็น 0 และทำการปรับขนาดให้เป็นช่วง 0-255
			\item เก็บข้อมูลออกมาในรูปแบบของ grayscale image เพื่อลดพื้นที่ ๆ ใช้เก็บข้อมูล
			\item แก้ปัญหาเรื่องการเคลื่อนไหวของกล้อง(camera motion) โดยการนำค่าเฉลี่ยของ เวกเตอร์(vector) ไปลบกับ displacement
			\item สุดท้ายจะเป็นสุ่มตัดภาพออกมาเพื่อเพิ่มจำนวนข้อมูล
		\end{enumerate}
	\item {โมเดลปัญญาประดิษฐ์}
	\begin{enumerate}
		\item ในงานวิจัยนี้มีการทดสอบ โมเดลปัญญาประดิษฐ์ หลายอัน ซึ่ง โมเดลปัญญาประดิษฐ์ ที่มีประสิทธิภาพการทำงานที่ดีที่สุดตาม 5 ลำดับแรกดังนี้
			\begin{enumerate}
				\item SVM				มีรูปแบบข้อมูลอินพุท คือ Spatial+Temporal+Auditory 	
				\item I3D 				มีรูปแบบข้อมูลอินพุท คือ Spatial+Temporal
				\item TRN-Multiscale		มีรูปแบบข้อมูลอินพุท คือ Spatial+Temporal
				\item TSN-2stream		มีรูปแบบข้อมูลอินพุท คือ Spatial+Temporal
				\item ResNet50-ImageNet	มีรูปแบบข้อมูลอินพุท คือ Spatial
			\end{enumerate}
		\item เครื่องมือที่ใช้วัดผลงานวิจัยนี้
			\begin{enumerate}
				\item Classification accuracy Top-1 , Top-5
			\end{enumerate}
		\item ความสามารถของ โมเดลปัญญาประดิษฐ์ ในปัจจุบัน
			\begin{enumerate}
				\item ทำทดสอบ cross dataset transfer โดยการนำโมเดล ResNet50 I3D pretrained ลงทั้งบน Kinetics และ Moments in time และนำมาเทียบกับชุดข้อมูลอื่น โดยชุดข้อมูลแต่ละชุดจะมีการปรับ frame rate ของวิดีโอให้เป็น 5 fps เหมือนกัน
	\begin{table}[!ht]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			{Pretrained}&\multicolumn{3}{c|}{Fine-Tuned}\\
			\cline{2-4}
			{}			& UCF		& HMDB		& Something			\\
			\hline
			\multirow{2}{*}{Kinetics}		& Top-1 : 92.6		& Top-1 : 62.0		& Top-1 : 48.6		\\
			{}						& Top-5 : 99.2		& Top-5 : 88.2		& Top-5 : 77.9		\\
			\hline
			\multirow{2}{*}{Moments}		& Top-1 : 91.9		& Top-1 : 65.9		& Top-1 : 50.0		\\
			{}						& Top-5 : 98.6		& Top-5 : 89.3		& Top-5 : 78.8		\\
			\hline
		\end{tabular}
		\caption{Data transfer performance ของโมเดล Resnet50 I3D}
		\label{tab: Data transfer performance ของโมเดล Resnet50 I3D}
	\end{table}
				\item จะเห็นได้ว่า Kinetics ให้ผลลัพท์ที่ดีกว่าใน UCF เพราะว่ามีการแชร์ class ด้วยกันอยู่หลายอย่าง ในขณะที่ HMDB นั้นมีการรวบรวม source จากหลายแหล่ง และมีจำนวน class ที่หลากหลายจึงทำให้มีความใกล้เคียงกับตัวข้อมูลของ Moments in time ดังนั้นจึงเทียบผลลัพท์จาก Something ซึ่งจะทำให้เห็นว่า Moments in time มีประสิทธิภาพที่ดีกว่าและวิดีโอที่มีความยาวมากกว่า 3 วินาทีจะไม่ส่งผลกระทบกับประสิทธิภาพของ Moments in time
		\end{enumerate}
	\end{enumerate}
	\clearpage
	\item {ปัญหาที่พบ}
	\begin{enumerate}
		\item ผลลัพท์จากการทำนายด้วยโมเดลถ้าผ่านรูปภาพที่มีรายละเอียดเยอะจะทำให้การ ทำนายโอกาสผิดนั้นค่อนข้างสูง ซึ่งปัญหานี่สามารถทำให้เกิดน้อยลงด้วยการนำวิธี Class Activation Mapping(CAM) จะเป็นการเน้นรูปภาพในส่วนที่มีข้อมูลมากที่สุดและ ทำนายผลออกมา แต่ก็ยังมีจุดที่เป็นปัญหาอยู่ เช่น การกระที่เกิดขึ้นเร็วมาก (การลื่นล้ม) จะทำให้การทำนาย นั้นมีโอกาสผิดสูงขึ้น 
	\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{enumerate}		